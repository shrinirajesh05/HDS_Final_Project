{
 "cells": [
  {
   "cell_type": "raw",
   "id": "468037a6-6822-4b0f-92ea-c7130545261e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Packages and Modules Required\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !pip install --upgrade imbalanced-learn scikit-learn\n",
    "# !pip install -U scikit-learn imbalanced-learn\n",
    "# !pip install xgboost\n",
    "# !pip install --upgrade bokeh\n",
    "# !pip install tabulate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.layouts import column\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from bokeh.io import export\n",
    "from tabulate import tabulate\n",
    "from bokeh.plotting import output_notebook\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.palettes import Category20b\n",
    "#! pip install pygam\n",
    "from pygam import LogisticGAM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41816c0f-7b7b-40a1-9c1a-91dc664cc4f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the dataset\n",
    "file_path = 'diabetes_CDC.csv'\n",
    "d_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(d_df.head())\n",
    "d_df.columns\n",
    "d_df.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7fa24d3-8aa5-4986-86f2-b8e62a9cf588",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis \n",
    "# Check for missing values\n",
    "missing_values_count = d_df.isnull().sum()\n",
    "print(\"Missing values count:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter columns with missing values > 0\n",
    "missing_columns = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values > 0\n",
    "if not missing_columns.empty:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_columns)\n",
    "else:\n",
    "    print(\"No missing values found in any column.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c392c32-b1cc-422b-a941-ed97eae10bb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis - Contd.\n",
    "# Get the data types of all variables\n",
    "variable_types = d_df.dtypes\n",
    "\n",
    "# Print the data types\n",
    "print(\"Data types of all variables:\")\n",
    "print(variable_types)\n",
    "\n",
    "# Binary variables represented as integers (0 or 1) can be used as int type.\n",
    "# Converting Catrgorical Variable types from int to category\n",
    "\n",
    "# Convert ordinal variables to categorical factors\n",
    "d_df['MentHlth'] = d_df['MentHlth'].astype('category')\n",
    "d_df['PhysHlth'] = d_df['PhysHlth'].astype('category')\n",
    "d_df['Education'] = d_df['Education'].astype('category')\n",
    "d_df['Income'] = d_df['Income'].astype('category')\n",
    "d_df['Age'] = d_df['Age'].astype('category')\n",
    "# Replace values in 'GenHlth' column with their inverses\n",
    "d_df['GenHlth'] = d_df['GenHlth'].replace({1: 5, 2: 4, 3: 3, 4: 2, 5: 1})\n",
    "\n",
    "# Display the modified 'GenHlth' column\n",
    "print(d_df['GenHlth'])\n",
    "\n",
    "print(d_df['GenHlth'].head())\n",
    "d_df['GenHlth'] = d_df['GenHlth'].astype('category')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(d_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c5fc0a-2756-4f61-9fde-4f5b3224ca47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis - Contd\n",
    "# Get the data types of all variables\n",
    "variable_types = d_df.dtypes\n",
    "\n",
    "# Print the data types\n",
    "print(\"Data types of all variables:\")\n",
    "print(variable_types)\n",
    "print(d_df['GenHlth'])\n",
    "\n",
    "\n",
    "# Assuming d_df is your DataFrame and it contains the following columns\n",
    "selected_columns = ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP', 'target']\n",
    "\n",
    "# Extract the relevant columns\n",
    "selected_df = d_df[selected_columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "selected_df.to_csv('selected_features.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfad77eb-16bb-4611-8c61-db5f1dedbcf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis - Contd\n",
    "# Get unique values of the target variable\n",
    "unique_targets = d_df['target'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values of the target variable:\")\n",
    "print(unique_targets)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6faae0aa-b814-40a8-a635-205939c376f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis - Contd\n",
    "# Compute the correlation matrix\n",
    "corr = d_df.corr(numeric_only=True)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool_))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "x = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Show Plot\n",
    "print(\"---CORRELATIONS---\")\n",
    "print(corr)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bae60200-3403-4220-8e50-43fb7a2efde0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis - Contd\n",
    "# Compute the correlation matrix\n",
    "corr = d_df.corr(numeric_only=True)\n",
    "\n",
    "# Get the correlation values with the target variable\n",
    "target_corr = corr['target'].drop('target')\n",
    "\n",
    "# Print the correlation values with the target variable\n",
    "print(\"Correlation with target variable:\")\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40ea7ace-fe9c-4a09-b542-6b9b6fb10acb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Selection\n",
    "# Create a VarianceThreshold object with a threshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "# Fit and transform the data to remove low variance features\n",
    "X_high_variance = selector.fit_transform(d_df.drop(columns=['target']))\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = d_df.drop(columns=['target']).columns[selector.get_support()]\n",
    "\n",
    "\n",
    "# Print the total number of features\n",
    "total_features = len(d_df.drop(columns=['target']).columns)\n",
    "print(\"Total features:\", total_features)\n",
    "\n",
    "# Print the count of selected features\n",
    "selected_features_count = len(selected_features)\n",
    "print(\"Number of selected features after applying low variance filter:\", selected_features_count)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features after applying low variance filter:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bc04adc-4bf7-41e4-b438-21f835198a9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Selection - Contd.\n",
    "# Create a VarianceThreshold object with a threshold\n",
    "selector = VarianceThreshold(threshold=0.5)\n",
    "\n",
    "# Fit and transform the data to remove low variance features\n",
    "X_high_variance = selector.fit_transform(d_df.drop(columns=['target']))\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = d_df.drop(columns=['target']).columns[selector.get_support()]\n",
    "\n",
    "\n",
    "# Print the total number of features\n",
    "total_features = len(d_df.drop(columns=['target']).columns)\n",
    "print(\"Total features:\", total_features)\n",
    "\n",
    "# Print the count of selected features\n",
    "selected_features_count = len(selected_features)\n",
    "print(\"Number of selected features after applying low variance filter:\", selected_features_count)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features after applying low variance filter:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcf5a2dd-52e1-4a7a-ab3b-f704f9a0054b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Selection - Contd.\n",
    "# Feature selection using SelectFromModel with RandomForestClassifier\n",
    "# This method selects features based on their importance as determined by a Random Forest Classifier.\n",
    "\n",
    "# Define your dataset (features and target)\n",
    "feat = d_df.drop(columns=['target'])\n",
    "tar = d_df['target']\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=3, criterion='entropy', random_state=50)\n",
    "\n",
    "#LogisticRegression(max_iter=1000, random_state=50)\n",
    "\n",
    "# Create the SelectFromModel object\n",
    "sel = SelectFromModel(estimator=clf, prefit=False, threshold='mean')\n",
    "\n",
    "# Fit the SelectFromModel object to the dataset\n",
    "sel.fit(feat, tar)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = feat.columns[sel.get_support()]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features using SelectFromModel with Random Forest Classifier:\")\n",
    "print(\"Total features:\", len(feat.columns))\n",
    "print(\"Number of selected features:\", len(selected_features))\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de85586d-ecc3-4a9d-afbc-4917b41f84c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Selection - Contd.\n",
    "# Assigning features based on the two feature selection methods\n",
    "selected_features_rf = ['HighBP', 'BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "selected_features_lv = ['HighBP', 'HighChol', 'BMI', 'Smoker', 'PhysActivity', 'Fruits',\n",
    "                        'Veggies', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age',\n",
    "                        'Education', 'Income']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c81a855a-2ae5-4084-b0ec-607a7cd05325",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EDA Contd.\n",
    "# Desciprtive Statistics\n",
    "\n",
    "# Select the subset of data containing only the selected features\n",
    "selected_features_data = d_df[selected_features_rf]\n",
    "print(selected_features_data.dtypes)\n",
    "\n",
    "print(d_df.dtypes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2adfef68-d299-4460-9b85-d205f9c7793a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EDA Contd.\n",
    "# Summary statistics for binary and categorical columns\n",
    "summary_tables = []\n",
    "\n",
    "for column in d_df.columns:\n",
    "    if column == 'BMI':\n",
    "        # Use describe for numerical column 'BMI'\n",
    "        bmi_summary = d_df['BMI'].describe().reset_index()\n",
    "        bmi_summary.columns = ['Statistic', 'Value']\n",
    "        summary_tables.append(('Summary statistics for BMI column:', tabulate(bmi_summary, tablefmt='pretty', headers='keys', showindex=False)))\n",
    "    else:\n",
    "        # Count unique values and find mode for other columns\n",
    "        counts = d_df[column].value_counts().reset_index()\n",
    "        counts.columns = ['Value', 'Counts']\n",
    "        mode = d_df[column].mode()[0]\n",
    "        summary_tables.append((f\"Summary statistics for '{column}' column:\",\n",
    "                               tabulate(counts, tablefmt='pretty', headers='keys', showindex=False) +\n",
    "                               f\"\\nMode: {mode}\"))\n",
    "\n",
    "# Print summary tables\n",
    "for title, table in summary_tables:\n",
    "    print(title)\n",
    "    print(table)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcd6f930-6124-40aa-b8fe-9347bb1764b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EDA Contd.\n",
    "# Plot bar plot for 'GenHlth'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='GenHlth', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of General Health', fontsize=16)\n",
    "plt.xlabel('General Health', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'MentHlth'\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.countplot(x='MentHlth', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Mental Health', fontsize=16)\n",
    "plt.xlabel('Mental Health', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'PhysHlth'\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.countplot(x='PhysHlth', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Physical Health', fontsize=16)\n",
    "plt.xlabel('Physical Health', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'Education'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Education', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Education', fontsize=16)\n",
    "plt.xlabel('Education', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'Income'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Income', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Income', fontsize=16)\n",
    "plt.xlabel('Income', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot bar plot for 'HighBP'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='HighBP', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of High Blood Pressure', fontsize=16)\n",
    "plt.xlabel('High Blood Pressure', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks([0, 1], ['No', 'Yes'], fontsize=12)  # Replace 0 and 1 with labels\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot bar plot for 'Age'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Age', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Age', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for 'BMI'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=selected_features_data, x='BMI', bins=20, color='skyblue')\n",
    "plt.title('Distribution of BMI', fontsize=16)\n",
    "plt.xlabel('BMI', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "582cead8-93ff-4351-97d9-616d79ef04fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split the Data Train and Test\n",
    "\n",
    "# all features\n",
    "\n",
    "# Separate features and target\n",
    "diabetes_X = d_df.drop(columns=['target'])\n",
    "print(diabetes_X.columns)\n",
    "diabetes_Y = d_df['target'] \n",
    "print(diabetes_Y.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(diabetes_X, diabetes_Y, test_size=0.2, random_state=55)\n",
    "# Display the shape of the training and testing sets\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "# selected_features_rf\n",
    "# Separate features and target\n",
    "rf_X = d_df[selected_features_rf]\n",
    "print(rf_X.columns)\n",
    "rf_Y = d_df['target'] \n",
    "print(rf_Y.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_rf, X_test_rf, Y_train_rf, Y_test_rf = train_test_split(rf_X, rf_Y, test_size=0.2, random_state=55)\n",
    "# Display the shape of the training and testing sets\n",
    "print(X_train_rf.shape, X_test_rf.shape, Y_train_rf.shape, Y_test_rf.shape)\n",
    "\n",
    "# selected_features_lv\n",
    "# Separate features and target\n",
    "lv_X = d_df[selected_features_lv]\n",
    "print(lv_X.columns)\n",
    "lv_Y = d_df['target'] \n",
    "print(lv_Y.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_lv, X_test_lv, Y_train_lv, Y_test_lv = train_test_split(lv_X, lv_Y, test_size=0.2, random_state=55)\n",
    "# Display the shape of the training and testing sets\n",
    "print(X_train_lv.shape, X_test_lv.shape, Y_train_lv.shape, Y_test_lv.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa275c73-31f9-44e3-862a-5915e861d672",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "# Logistic Regression on three sets of features\n",
    "\n",
    "# all features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_all = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_all.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg = logreg_all.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred_logreg))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(Y_test, y_pred_logreg)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision = precision_score(Y_test, y_pred_logreg, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall = recall_score(Y_test, y_pred_logreg, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score = f1_score(Y_test, y_pred_logreg, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy:\", overall_accuracy)\n",
    "print(\"Overall Precision:\", overall_precision)\n",
    "print(\"Overall Recall:\", overall_recall)\n",
    "print(\"Overall F1-Score:\", overall_f1_score)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred_logreg)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_rf\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_rf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_rf.fit(X_train_rf, Y_train_rf)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg_rf = logreg_rf.predict(X_test_rf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Random Forest:\\n\", classification_report(Y_test_rf, y_pred_logreg_rf))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_rf = accuracy_score(Y_test_rf, y_pred_logreg_rf)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_rf = precision_score(Y_test_rf, y_pred_logreg_rf, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_rf = recall_score(Y_test_rf, y_pred_logreg_rf, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_rf = f1_score(Y_test_rf, y_pred_logreg_rf, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Random Forest:\", overall_accuracy_rf)\n",
    "print(\"Overall Precision for selected features using Random Forest:\", overall_precision_rf)\n",
    "print(\"Overall Recall for selected features using Random Forest:\", overall_recall_rf)\n",
    "print(\"Overall F1-Score for selected features using Random Forest:\", overall_f1_score_rf)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(Y_test_rf, y_pred_logreg_rf)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf = pd.DataFrame(conf_matrix_rf, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Random Forest:\")\n",
    "print(confusion_df_rf)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_lv\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_lv = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_lv.fit(X_train_lv, Y_train_lv)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg_lv = logreg_lv.predict(X_test_lv)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Low Variance:\\n\", classification_report(Y_test_lv, y_pred_logreg_lv))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_lv = accuracy_score(Y_test_lv, y_pred_logreg_lv)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_lv = precision_score(Y_test_lv, y_pred_logreg_lv, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_lv = recall_score(Y_test_lv, y_pred_logreg_lv, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_lv = f1_score(Y_test_lv, y_pred_logreg_lv, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Low Variance:\", overall_accuracy_lv)\n",
    "print(\"Overall Precision for selected features using Low Variance:\", overall_precision_lv)\n",
    "print(\"Overall Recall for selected features using Low Variance:\", overall_recall_lv)\n",
    "print(\"Overall F1-Score for selected features using Low Variance:\", overall_f1_score_lv)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_lv = confusion_matrix(Y_test_lv, y_pred_logreg_lv)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv = pd.DataFrame(conf_matrix_lv, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Low Variance:\")\n",
    "print(confusion_df_lv)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea206ba5-9af9-4c11-8875-9f8cfff7eace",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EDA Contd.\n",
    "# Checking for class imbalance\n",
    "\n",
    "# Calculate the proportion of each class in the target variable\n",
    "class_counts = d_df['target'].value_counts()\n",
    "class_proportions = class_counts / class_counts.sum()\n",
    "\n",
    "# Print the class proportions\n",
    "print(\"Class Proportions:\")\n",
    "print(class_proportions)\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4027f48-6b0b-4ea7-aa35-41312cddc15d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Resampling using SMOTE to avoid class imbalance\n",
    "# Apply SMOTE to the training data only\n",
    "\n",
    "# all features\n",
    "smote = SMOTE(random_state=55)\n",
    "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
    "print(X_train.shape, Y_train.shape, X_train_resampled.shape,Y_train_resampled.shape)\n",
    "\n",
    "# Checking the Resampled Counts\n",
    "\n",
    "# Assuming X_train_resampled and Y_train_resampled are NumPy arrays or pandas DataFrames/Series\n",
    "# Convert them to pandas DataFrame/Series if they are not already\n",
    "\n",
    "# Convert to DataFrame if they are numpy arrays\n",
    "X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "# Convert to Series if they are numpy arrays\n",
    "Y_train_resampled_series = pd.Series(Y_train_resampled)\n",
    "\n",
    "# Create a DataFrame with X_train_resampled and Y_train_resampled\n",
    "resampled_df = pd.concat([X_train_resampled_df, Y_train_resampled_series], axis=1)\n",
    "\n",
    "# Calculate the proportion of each class in the target variable for resampled data\n",
    "resampled_class_counts = resampled_df['target'].value_counts()\n",
    "resampled_class_proportions = resampled_class_counts / resampled_class_counts.sum()\n",
    "\n",
    "# Print the class counts and proportions for resampled data\n",
    "print(\"Resampled Class Counts:\")\n",
    "print(resampled_class_counts)\n",
    "print(\"\\nResampled Class Proportions:\")\n",
    "print(resampled_class_proportions)\n",
    "\n",
    "# rf\n",
    "# Apply SMOTE to the training data only\n",
    "smote = SMOTE(random_state=55)\n",
    "X_train_rf_resampled, Y_train_rf_resampled = smote.fit_resample(X_train_rf, Y_train_rf)\n",
    "print(X_train_rf.shape, Y_train_rf.shape, X_train_rf_resampled.shape,Y_train_rf_resampled.shape)\n",
    "\n",
    "#lv\n",
    "# Apply SMOTE to the training data only\n",
    "smote = SMOTE(random_state=55)\n",
    "X_train_lv_resampled, Y_train_lv_resampled = smote.fit_resample(X_train_lv, Y_train_lv)\n",
    "print(X_train_lv.shape, Y_train_lv.shape, X_train_lv_resampled.shape,Y_train_lv_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21d6e4f8-f82c-4ef4-8ed1-c3383bb54232",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "# Logistic Regression on resampled data\n",
    "# all features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_all_resampled = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "logreg_all_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg_resampled = logreg_all_resampled.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for all features using resampled data:\\n\", classification_report(Y_test, y_pred_logreg_resampled))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_resampled = accuracy_score(Y_test, y_pred_logreg_resampled)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_resampled = precision_score(Y_test, y_pred_logreg_resampled, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_resampled = recall_score(Y_test, y_pred_logreg_resampled, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_resampled = f1_score(Y_test, y_pred_logreg_resampled, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for all features using resampled data:\", overall_accuracy_resampled)\n",
    "print(\"Overall Precision for all features using resampled data:\", overall_precision_resampled)\n",
    "print(\"Overall Recall for all features using resampled data:\", overall_recall_resampled)\n",
    "print(\"Overall F1-Score for all features using resampled data:\", overall_f1_score_resampled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_resampled = confusion_matrix(Y_test, y_pred_logreg_resampled)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_resampled = pd.DataFrame(conf_matrix_resampled, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for all features using resampled data:\")\n",
    "print(confusion_df_resampled)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_rf\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_rf_resampled = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "logreg_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "y_pred_logreg_rf_resampled = logreg_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Random Forest with resampled data:\\n\", classification_report(Y_test_rf, y_pred_logreg_rf_resampled))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_rf_resampled = accuracy_score(Y_test_rf, y_pred_logreg_rf_resampled)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_rf_resampled = precision_score(Y_test_rf, y_pred_logreg_rf_resampled, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_rf_resampled = recall_score(Y_test_rf, y_pred_logreg_rf_resampled, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_rf_resampled = f1_score(Y_test_rf, y_pred_logreg_rf_resampled, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Random Forest with resampled data:\", overall_accuracy_rf_resampled)\n",
    "print(\"Overall Precision for selected features using Random Forest with resampled data:\", overall_precision_rf_resampled)\n",
    "print(\"Overall Recall for selected features using Random Forest with resampled data:\", overall_recall_rf_resampled)\n",
    "print(\"Overall F1-Score for selected features using Random Forest with resampled data:\", overall_f1_score_rf_resampled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_rf_resampled = confusion_matrix(Y_test_rf, y_pred_logreg_rf_resampled)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled = pd.DataFrame(conf_matrix_rf_resampled, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Random Forest with resampled data:\")\n",
    "print(confusion_df_rf_resampled)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_lv\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_lv_resampled = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "logreg_lv_resampled.fit(X_train_lv_resampled, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "y_pred_logreg_lv_resampled = logreg_lv_resampled.predict(X_test_lv)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Low Variance with resampled data:\\n\", classification_report(Y_test_lv, y_pred_logreg_lv_resampled))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_lv_resampled = accuracy_score(Y_test_lv, y_pred_logreg_lv_resampled)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_lv_resampled = precision_score(Y_test_lv, y_pred_logreg_lv_resampled, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_lv_resampled = recall_score(Y_test_lv, y_pred_logreg_lv_resampled, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_lv_resampled = f1_score(Y_test_lv, y_pred_logreg_lv_resampled, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Low Variance with resampled data:\", overall_accuracy_lv_resampled)\n",
    "print(\"Overall Precision for selected features using Low Variance with resampled data:\", overall_precision_lv_resampled)\n",
    "print(\"Overall Recall for selected features using Low Variance with resampled data:\", overall_recall_lv_resampled)\n",
    "print(\"Overall F1-Score for selected features using Low Variance with resampled data:\", overall_f1_score_lv_resampled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_lv_resampled = confusion_matrix(Y_test_lv, y_pred_logreg_lv_resampled)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv_resampled = pd.DataFrame(conf_matrix_lv_resampled, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Low Variance with resampled data:\")\n",
    "print(confusion_df_lv_resampled)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acfa3480-7c46-4051-8c6d-88b90626c54b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Best model of the above three LR\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_resampled = {\n",
    "    'all_features_resampled': {'accuracy': overall_accuracy_resampled, 'precision': overall_precision_resampled, \n",
    "                     'recall': overall_recall_resampled, 'f1_score': overall_f1_score_resampled},\n",
    "    'selected_features_rf_resampled': {'accuracy': overall_accuracy_rf_resampled, 'precision': overall_precision_rf_resampled, \n",
    "                             'recall': overall_recall_rf_resampled, 'f1_score': overall_f1_score_rf_resampled},\n",
    "    'selected_features_lv_resampled': {'accuracy': overall_accuracy_lv_resampled, 'precision': overall_precision_lv_resampled, \n",
    "                             'recall': overall_recall_lv_resampled, 'f1_score': overall_f1_score_lv_resampled}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_resampled.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics \n",
    "weights_resampled = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_resampled = {}\n",
    "for model, metrics in models_evaluation_resampled.items():\n",
    "    weighted_sum = sum(metrics[metric] * weights_resampled[metric] for metric in metrics)\n",
    "    weighted_sums_resampled[model] = weighted_sum\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_resampled = sorted(weighted_sums_resampled, key=weighted_sums_resampled.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_resampled, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_resampled[model]})\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8c51e13-2aab-4309-b7d8-e14243f63039",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "# Decision Tree on the Resampled Data\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  \n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# all features using resampled data\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_all_resampled = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "dt_all_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred_all_resampled_dt = dt_all_resampled.predict(X_test)\n",
    "\n",
    "# For all features using resampled data\n",
    "accuracy_all_resampled_dt = accuracy_score(Y_test, Y_pred_all_resampled_dt)\n",
    "precision_all_resampled_dt = precision_score(Y_test, Y_pred_all_resampled_dt, average='weighted')\n",
    "recall_all_resampled_dt = recall_score(Y_test, Y_pred_all_resampled_dt, average='weighted')\n",
    "f1_all_resampled_dt = f1_score(Y_test, Y_pred_all_resampled_dt, average='weighted')\n",
    "confusion_matrix_all_resampled_dt = confusion_matrix(Y_test, Y_pred_all_resampled_dt)\n",
    "\n",
    "print(\"For all features using resampled data (Decision Tree):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_all_resampled_dt))\n",
    "print(\"Overall Accuracy:\", accuracy_all_resampled_dt)\n",
    "print(\"Overall Precision:\", precision_all_resampled_dt)\n",
    "print(\"Overall Recall:\", recall_all_resampled_dt)\n",
    "print(\"Overall F1-Score:\", f1_all_resampled_dt)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_all_resampled_dt = pd.DataFrame(confusion_matrix_all_resampled_dt, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_all_resampled_dt)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_rf_resampled = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "dt_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_dt = dt_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_dt = accuracy_score(Y_test_rf, Y_pred_rf_resampled_dt)\n",
    "precision_rf_resampled_dt = precision_score(Y_test_rf, Y_pred_rf_resampled_dt, average='weighted')\n",
    "recall_rf_resampled_dt = recall_score(Y_test_rf, Y_pred_rf_resampled_dt, average='weighted')\n",
    "f1_rf_resampled_dt = f1_score(Y_test_rf, Y_pred_rf_resampled_dt, average='weighted')\n",
    "confusion_matrix_rf_resampled_dt = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_dt)\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Decision Tree):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_dt))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_dt)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_dt)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_dt)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_dt)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_dt = pd.DataFrame(confusion_matrix_rf_resampled_dt, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_rf_resampled_dt)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using low variance with resampled data\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_lv_resampled = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "dt_lv_resampled.fit(X_train_lv_resampled, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_lv_resampled_dt = dt_lv_resampled.predict(X_test_lv)\n",
    "\n",
    "# For selected features using low variance with resampled data\n",
    "accuracy_lv_resampled_dt = accuracy_score(Y_test_lv, Y_pred_lv_resampled_dt)\n",
    "precision_lv_resampled_dt = precision_score(Y_test_lv, Y_pred_lv_resampled_dt, average='weighted')\n",
    "recall_lv_resampled_dt = recall_score(Y_test_lv, Y_pred_lv_resampled_dt, average='weighted')\n",
    "f1_lv_resampled_dt = f1_score(Y_test_lv, Y_pred_lv_resampled_dt, average='weighted')\n",
    "confusion_matrix_lv_resampled_dt = confusion_matrix(Y_test_lv, Y_pred_lv_resampled_dt)\n",
    "\n",
    "print(\"\\nFor selected features using low variance with resampled data (Decision Tree):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_lv, Y_pred_lv_resampled_dt))\n",
    "print(\"Overall Accuracy:\", accuracy_lv_resampled_dt)\n",
    "print(\"Overall Precision:\", precision_lv_resampled_dt)\n",
    "print(\"Overall Recall:\", recall_lv_resampled_dt)\n",
    "print(\"Overall F1-Score:\", f1_lv_resampled_dt)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv_resampled_dt = pd.DataFrame(confusion_matrix_lv_resampled_dt, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_lv_resampled_dt)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4292381-7941-4703-9ec6-e28ec1388abb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Best model of the above three Decision Tree models\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_dt = {\n",
    "    'all_features_dt': {'accuracy': accuracy_all_resampled_dt, 'precision': precision_all_resampled_dt, \n",
    "                        'recall': recall_all_resampled_dt, 'f1_score': f1_all_resampled_dt},\n",
    "    'selected_features_rf_dt': {'accuracy': accuracy_rf_resampled_dt, 'precision': precision_rf_resampled_dt, \n",
    "                                'recall': recall_rf_resampled_dt, 'f1_score': f1_rf_resampled_dt},\n",
    "    'selected_features_lv_dt': {'accuracy': accuracy_lv_resampled_dt, 'precision': precision_lv_resampled_dt, \n",
    "                                'recall': recall_lv_resampled_dt, 'f1_score': f1_lv_resampled_dt}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_dt.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Overall Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Overall Precision:\", metrics['precision'])\n",
    "    print(\"Overall Recall:\", metrics['recall'])\n",
    "    print(\"Overall F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics \n",
    "weights = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_dt = {}\n",
    "for model, metrics in models_evaluation_dt.items():\n",
    "    weighted_sum_dt = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    weighted_sums_dt[model] = weighted_sum_dt\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_dt = sorted(weighted_sums_dt, key=weighted_sums_dt.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_dt, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_dt[model]})\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a8526cf-068a-4887-9f2e-ae48a6ca6100",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "# Random Forest Model\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# All features using resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_all_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "rfc_all_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred_all_resampled_rfc = rfc_all_resampled.predict(X_test)\n",
    "\n",
    "# For all features using resampled data\n",
    "accuracy_all_resampled_rfc = accuracy_score(Y_test, Y_pred_all_resampled_rfc)\n",
    "precision_all_resampled_rfc = precision_score(Y_test, Y_pred_all_resampled_rfc, average='weighted')\n",
    "recall_all_resampled_rfc = recall_score(Y_test, Y_pred_all_resampled_rfc, average='weighted')\n",
    "f1_all_resampled_rfc = f1_score(Y_test, Y_pred_all_resampled_rfc, average='weighted')\n",
    "confusion_matrix_all_resampled_rfc = confusion_matrix(Y_test, Y_pred_all_resampled_rfc)\n",
    "\n",
    "print(\"For all features using resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_all_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_all_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_all_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_all_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_all_resampled_rfc)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_all_resampled_rfc = pd.DataFrame(confusion_matrix_all_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_all_resampled_rfc)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_rf_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "rfc_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_rfc = rfc_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_rfc = accuracy_score(Y_test_rf, Y_pred_rf_resampled_rfc)\n",
    "precision_rf_resampled_rfc = precision_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "recall_rf_resampled_rfc = recall_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "f1_rf_resampled_rfc = f1_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "confusion_matrix_rf_resampled_rfc = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_rfc)\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_rfc)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_rfc = pd.DataFrame(confusion_matrix_rf_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_rf_resampled_rfc)\n",
    "\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_rfc)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using low variance with resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_lv_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "rfc_lv_resampled.fit(X_train_lv_resampled, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_lv_resampled_rfc = rfc_lv_resampled.predict(X_test_lv)\n",
    "\n",
    "# For selected features using low variance with resampled data\n",
    "accuracy_lv_resampled_rfc = accuracy_score(Y_test_lv, Y_pred_lv_resampled_rfc)\n",
    "precision_lv_resampled_rfc = precision_score(Y_test_lv, Y_pred_lv_resampled_rfc, average='weighted')\n",
    "recall_lv_resampled_rfc = recall_score(Y_test_lv, Y_pred_lv_resampled_rfc, average='weighted')\n",
    "f1_lv_resampled_rfc = f1_score(Y_test_lv, Y_pred_lv_resampled_rfc, average='weighted')\n",
    "confusion_matrix_lv_resampled_rfc = confusion_matrix(Y_test_lv, Y_pred_lv_resampled_rfc)\n",
    "\n",
    "print(\"\\nFor selected features using low variance with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_lv, Y_pred_lv_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_lv_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_lv_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_lv_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_lv_resampled_rfc)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv_resampled_rfc = pd.DataFrame(confusion_matrix_lv_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_lv_resampled_rfc)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f2ad87e-68f0-4133-8ec8-6203036c9c70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Best model of the above three Random Forest models\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_rfc = {\n",
    "    'all_features_rfc': {'accuracy': accuracy_all_resampled_rfc, 'precision': precision_all_resampled_rfc, \n",
    "                         'recall': recall_all_resampled_rfc, 'f1_score': f1_all_resampled_rfc},\n",
    "    'selected_features_rf_rfc': {'accuracy': accuracy_rf_resampled_rfc, 'precision': precision_rf_resampled_rfc, \n",
    "                                 'recall': recall_rf_resampled_rfc, 'f1_score': f1_rf_resampled_rfc},\n",
    "    'selected_features_lv_rfc': {'accuracy': accuracy_lv_resampled_rfc, 'precision': precision_lv_resampled_rfc, \n",
    "                                 'recall': recall_lv_resampled_rfc, 'f1_score': f1_lv_resampled_rfc}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_rfc.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics \n",
    "weights = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_rfc = {}\n",
    "for model, metrics in models_evaluation_rfc.items():\n",
    "    weighted_sum_rfc = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    weighted_sums_rfc[model] = weighted_sum_rfc\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_rfc = sorted(weighted_sums_rfc, key=weighted_sums_rfc.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_rfc, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_rfc[model]})\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f1fc149-00ba-4e55-9a1f-ec9885f03434",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create new variables for resampled datasets\n",
    "X_train_resampled_new = X_train_resampled.copy()\n",
    "X_test_new = X_test.copy()\n",
    "X_train_rf_resampled_new = X_train_rf_resampled.copy()\n",
    "X_test_rf_new = X_test_rf.copy()\n",
    "X_train_lv_resampled_new = X_train_lv_resampled.copy()\n",
    "X_test_lv_new = X_test_lv.copy()\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "for col in categorical_columns:\n",
    "    X_train_resampled_new[col] = X_train_resampled_new[col].astype(float)\n",
    "    X_test_new[col] = X_test_new[col].astype(float)\n",
    "    X_train_rf_resampled_new[col] = X_train_rf_resampled_new[col].astype(float)\n",
    "    X_test_rf_new[col] = X_test_rf_new[col].astype(float)\n",
    "    X_train_lv_resampled_new[col] = X_train_lv_resampled_new[col].astype(float)\n",
    "    X_test_lv_new[col] = X_test_lv_new[col].astype(float)\n",
    "\n",
    "# Now you can proceed with training your models using these new variables\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7025acbb-3289-4872-bfb1-10d601b684d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "# Gradient Boosting\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# All features using resampled data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_all_resampled_new = XGBClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "xgb_all_resampled_new.fit(X_train_resampled_new, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred_all_resampled_xgb_new = xgb_all_resampled_new.predict(X_test_new)\n",
    "\n",
    "# For all features using resampled data\n",
    "accuracy_all_resampled_xgb_new = accuracy_score(Y_test, Y_pred_all_resampled_xgb_new)\n",
    "precision_all_resampled_xgb_new = precision_score(Y_test, Y_pred_all_resampled_xgb_new, average='weighted')\n",
    "recall_all_resampled_xgb_new = recall_score(Y_test, Y_pred_all_resampled_xgb_new, average='weighted')\n",
    "f1_all_resampled_xgb_new = f1_score(Y_test, Y_pred_all_resampled_xgb_new, average='weighted')\n",
    "confusion_matrix_all_resampled_xgb_new = confusion_matrix(Y_test, Y_pred_all_resampled_xgb_new)\n",
    "\n",
    "print(\"For all features using resampled data (XGBoost with new variables):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_all_resampled_xgb_new))\n",
    "print(\"Overall Accuracy:\", accuracy_all_resampled_xgb_new)\n",
    "print(\"Overall Precision:\", precision_all_resampled_xgb_new)\n",
    "print(\"Overall Recall:\", recall_all_resampled_xgb_new)\n",
    "print(\"Overall F1-Score:\", f1_all_resampled_xgb_new)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_all_resampled_xgb_new = pd.DataFrame(confusion_matrix_all_resampled_xgb_new, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_all_resampled_xgb_new)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_rf_resampled_new = XGBClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "xgb_rf_resampled_new.fit(X_train_rf_resampled_new, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_xgb_new = xgb_rf_resampled_new.predict(X_test_rf_new)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_xgb_new = accuracy_score(Y_test_rf, Y_pred_rf_resampled_xgb_new)\n",
    "precision_rf_resampled_xgb_new = precision_score(Y_test_rf, Y_pred_rf_resampled_xgb_new, average='weighted')\n",
    "recall_rf_resampled_xgb_new = recall_score(Y_test_rf, Y_pred_rf_resampled_xgb_new, average='weighted')\n",
    "f1_rf_resampled_xgb_new = f1_score(Y_test_rf, Y_pred_rf_resampled_xgb_new, average='weighted')\n",
    "confusion_matrix_rf_resampled_xgb_new = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_xgb_new)\n",
    "\n",
    "# Print metrics for selected features using Random Forest with resampled data\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (XGBoost):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_xgb_new))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_xgb_new)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_xgb_new)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_xgb_new)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_xgb_new)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix_rf_resampled_xgb_new, index=class_labels, columns=class_labels))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using low variance with resampled data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_lv_resampled_new = XGBClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "xgb_lv_resampled_new.fit(X_train_lv_resampled_new, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_lv_resampled_xgb_new = xgb_lv_resampled_new.predict(X_test_lv_new)\n",
    "\n",
    "# For selected features using low variance with resampled data\n",
    "accuracy_lv_resampled_xgb_new = accuracy_score(Y_test_lv, Y_pred_lv_resampled_xgb_new)\n",
    "precision_lv_resampled_xgb_new = precision_score(Y_test_lv, Y_pred_lv_resampled_xgb_new, average='weighted')\n",
    "recall_lv_resampled_xgb_new = recall_score(Y_test_lv, Y_pred_lv_resampled_xgb_new, average='weighted')\n",
    "f1_lv_resampled_xgb_new = f1_score(Y_test_lv, Y_pred_lv_resampled_xgb_new, average='weighted')\n",
    "confusion_matrix_lv_resampled_xgb_new = confusion_matrix(Y_test_lv, Y_pred_lv_resampled_xgb_new)\n",
    "\n",
    "# Print metrics for selected features using low variance with resampled data\n",
    "print(\"\\nFor selected features using low variance with resampled data (XGBoost):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_lv, Y_pred_lv_resampled_xgb_new))\n",
    "print(\"Overall Accuracy:\", accuracy_lv_resampled_xgb_new)\n",
    "print(\"Overall Precision:\", precision_lv_resampled_xgb_new)\n",
    "print(\"Overall Recall:\", recall_lv_resampled_xgb_new)\n",
    "print(\"Overall F1-Score:\", f1_lv_resampled_xgb_new)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix_lv_resampled_xgb_new, index=class_labels, columns=class_labels))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f92e2f84-3ef8-4f47-8404-e52e8c705851",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Best model of the above three XGBoost models\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_xgb = {\n",
    "    'all_features_xgb': {'accuracy': accuracy_all_resampled_xgb_new, 'precision': precision_all_resampled_xgb_new, \n",
    "                         'recall': recall_all_resampled_xgb_new, 'f1_score': f1_all_resampled_xgb_new},\n",
    "    'selected_features_rf_xgb': {'accuracy': accuracy_rf_resampled_xgb_new, 'precision': precision_rf_resampled_xgb_new, \n",
    "                                 'recall': recall_rf_resampled_xgb_new, 'f1_score': f1_rf_resampled_xgb_new},\n",
    "    'selected_features_lv_xgb': {'accuracy': accuracy_lv_resampled_xgb_new, 'precision': precision_lv_resampled_xgb_new, \n",
    "                                 'recall': recall_lv_resampled_xgb_new, 'f1_score': f1_lv_resampled_xgb_new}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_xgb.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics (you can adjust these weights based on your preference)\n",
    "weights = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_xgb = {}\n",
    "for model, metrics in models_evaluation_xgb.items():\n",
    "    weighted_sum_xgb = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    weighted_sums_xgb[model] = weighted_sum_xgb\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_xgb = sorted(weighted_sums_xgb, key=weighted_sums_xgb.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_xgb, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_xgb[model]})\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7aa8001b-0592-4d72-b598-697140390200",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics to confirm the differences\n",
    "# T test and ANOVA\n",
    "\n",
    "# T-tests\n",
    "t_stat, p_value = ttest_ind(d_df[d_df['HighBP'] == 1]['target'], d_df[d_df['HighBP'] == 0]['target'])\n",
    "\n",
    "# Print T-statistic summary\n",
    "print(\"T-test Summary:\")\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference in the mean values of the target variable (diabetes) between the groups with high blood pressure and without high blood pressure.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in the mean values of the target variable (diabetes) between the groups with high blood pressure and without high blood pressure.\")\n",
    "\n",
    "# T-test for BMI\n",
    "t_stat_bmi, p_value_bmi = ttest_ind(d_df[d_df['BMI'] >= 25]['target'], d_df[d_df['BMI'] < 25]['target'])\n",
    "\n",
    "# Print T-statistic summary for BMI\n",
    "print(\"T-test Summary for BMI:\")\n",
    "print(f\"T-statistic: {t_stat_bmi}\")\n",
    "print(f\"P-value: {p_value_bmi}\")\n",
    "if p_value_bmi < 0.05:\n",
    "    print(\"There is a statistically significant difference in the mean values of the target variable (diabetes) between individuals with BMI greater than or equal to 25 and those with BMI less than 25.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in the mean values of the target variable (diabetes) between individuals with BMI greater than or equal to 25 and those with BMI less than 25.\")\n",
    "\n",
    "# Define a function to perform ANOVA and print the summary\n",
    "def perform_anova(variable_name):\n",
    "    groups = [group['target'] for name, group in d_df.groupby(variable_name)]\n",
    "    f_stat, p_value_anova = f_oneway(*groups)\n",
    "    print(f\"ANOVA Summary for {variable_name}:\")\n",
    "    print(f\"F-statistic: {f_stat}\")\n",
    "    print(f\"P-value: {p_value_anova}\")\n",
    "    if p_value_anova < 0.05:\n",
    "        print(f\"There is a statistically significant difference in the mean values of the target variable (diabetes) across different levels of {variable_name}.\")\n",
    "    else:\n",
    "        print(f\"There is no statistically significant difference in the mean values of the target variable (diabetes) across different levels of {variable_name}.\")\n",
    "\n",
    "# Perform ANOVA for each categorical variable\n",
    "categorical_variables = ['GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education']\n",
    "for var in categorical_variables:\n",
    "    perform_anova(var)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7152407d-1672-4308-8b81-a9efea209708",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Best Model of 4 - Raandom Forest\n",
    "# Extract feature importances\n",
    "feature_importances_rf = rfc_rf_resampled.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': X_train_rf_resampled.columns, 'Importance': feature_importances_rf})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance for Random Forest with selected features:\")\n",
    "print(feature_importance_df_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df_rf['Feature'], feature_importance_df_rf['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for Random Forest with selected features')\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_rfc = pd.DataFrame(confusion_matrix_rf_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for Random Forest with selected features:\")\n",
    "print(confusion_df_rf_resampled_rfc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_rf_resampled_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Random Forest with selected features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd8722e6-aeac-47fa-8841-44c6d3d1affe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Perform k-fold cross-validation to be sure of the accuracy of the Random Forest Model\n",
    "cv_scores = cross_val_score(rfc_rf_resampled, X_train_rf_resampled, Y_train_rf_resampled, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "print(\"Mean accuracy:\", cv_scores.mean())\n",
    "print(\"Standard deviation of accuracy:\", cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96090832-f562-4a6d-a678-1fe6d708482a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interactive Dashboard on the Output of Random Forest and the Feature Relationship with Target \n",
    "# Using Bokeh\n",
    "\n",
    "# Define a custom color palette\n",
    "custom_palette = Category20b[20]\n",
    "custom_palette1 = Category20[20]\n",
    "\n",
    "# For each feature, assign a unique color from the custom palette for target 0 and target 1\n",
    "color_mapping = {\n",
    "    'BMI': [custom_palette[1], custom_palette1[1]],  # colors for target 0 and target 1 respectively\n",
    "    'GenHlth': [custom_palette[2], custom_palette1[2]],\n",
    "    'Age': [custom_palette[13], custom_palette1[3]],\n",
    "    'Income': [custom_palette[14], custom_palette1[4]],\n",
    "    'PhysHlth': [custom_palette[15], custom_palette1[5]],\n",
    "    'MentHlth': [custom_palette[16], custom_palette1[6]],  # Example, using the same colors for different features\n",
    "    'Education': [custom_palette[17], custom_palette1[7]],  # Example, using the same colors for different features\n",
    "    'HighBP': [custom_palette[18], custom_palette1[8]]  # Example, using the same colors for different features\n",
    "}\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Feature importance DataFrame\n",
    "feature_importance_data = {\n",
    "    'Feature': ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP'],\n",
    "    'Importance': [0.228, 0.198, 0.159, 0.111, 0.101, 0.078, 0.062, 0.058]\n",
    "}\n",
    "feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "\n",
    "# Assign a color palette to each feature\n",
    "colors = Category10[8]\n",
    "\n",
    "# Round off importance values to three decimals\n",
    "feature_importance_df['Importance'] = feature_importance_df['Importance']\n",
    "\n",
    "# Create a new column for colors in the DataFrame\n",
    "feature_importance_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(feature_importance_df)\n",
    "print(\"\\nPlease feel free to Hover over the plots.\\n\")\n",
    "# Create a figure for feature importance\n",
    "p_importance = figure(x_range=feature_importance_df['Feature'], height=400, width=500, title=\"Random Forest Model Features and Target Results\\nFeature Importance\",\n",
    "                      toolbar_location=None, tools=\"\")\n",
    "p_importance.vbar(x='Feature', top='Importance', width=0.9, source=source, \n",
    "                  fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Feature\", \"@Feature\"), (\"Importance\", \"@Importance\")]\n",
    "p_importance.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting to the feature importance plot\n",
    "p_importance.xaxis.major_label_orientation = 1.2\n",
    "p_importance.xgrid.grid_line_color = None\n",
    "p_importance.y_range.start = 0\n",
    "p_importance.yaxis.axis_label = \"Importance\"\n",
    "p_importance.xaxis.axis_label = \"Feature\"\n",
    "\n",
    "# Example data for demonstration\n",
    "scores = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [0.758, 0.823, 0.758, 0.783]\n",
    "}\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Define colors using the Category10 palette\n",
    "colors = Category10[4]\n",
    "\n",
    "# Add the 'Color' column to the DataFrame\n",
    "scores_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(scores_df)\n",
    "\n",
    "# Create a figure for model performance metrics\n",
    "p_score = figure(x_range=scores_df['Metric'], height=400, width=500, title=\"Model Performance Metrics\",\n",
    "                 toolbar_location=None, tools=\"\")\n",
    "\n",
    "# Plot the vertical bars with colors referenced from the data source\n",
    "p_score.vbar(x='Metric', top='Score', width=0.9, source=source, \n",
    "             fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Metric\", \"@Metric\"), (\"Score\", \"@Score\")]\n",
    "p_score.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting to the model performance metrics plot\n",
    "p_score.xaxis.major_label_orientation = 1.2\n",
    "p_score.xgrid.grid_line_color = None\n",
    "p_score.y_range.start = 0\n",
    "p_score.yaxis.axis_label = \"Score\"\n",
    "p_score.xaxis.axis_label = \"Metric\"\n",
    "\n",
    "# Assuming you have a DataFrame 'd_df' containing the data\n",
    "# d_df should have columns for each feature and a 'target' column\n",
    "\n",
    "# Define the list of features\n",
    "features = ['BMI', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP', 'GenHlth']\n",
    "\n",
    "# Create a list to store all the plots\n",
    "plots = []\n",
    "\n",
    "def plot_feature(feature):\n",
    "    if feature == 'BMI':\n",
    "        # Group data by 'target' column\n",
    "        group_0 = d_df[d_df['target'] == 0][feature]\n",
    "        group_1 = d_df[d_df['target'] == 1][feature]\n",
    "\n",
    "        # Create histogram for target 0\n",
    "        hist_0, edges_0 = np.histogram(group_0, bins=20)\n",
    "        p_hist = figure(title=f\"{feature} Distribution\", background_fill_color=\"#fafafa\", height=200, width=400)\n",
    "        p_hist.quad(top=hist_0, bottom=0, left=edges_0[:-1], right=edges_0[1:], fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "        # Create histogram for target 1\n",
    "        hist_1, edges_1 = np.histogram(group_1, bins=20)\n",
    "        p_hist.quad(top=hist_1, bottom=0, left=edges_1[:-1], right=edges_1[1:], fill_color=\"red\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "        # Add hover tooltips\n",
    "        hover = HoverTool()\n",
    "        hover.tooltips = [(\"Count\", \"@top\")]\n",
    "        p_hist.add_tools(hover)\n",
    "\n",
    "        # Set labels and axis\n",
    "        p_hist.xaxis.axis_label = feature\n",
    "        p_hist.yaxis.axis_label = 'Frequency'\n",
    "\n",
    "        return p_hist\n",
    "\n",
    "    \n",
    "    else:\n",
    "        color = color_mapping[feature]\n",
    "        # Define custom colors for each feature\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "        p = figure(x_range=sorted(map(str, d_df[feature].unique()), key=lambda x: int(x)), height=200, width=400,\n",
    "                   title=f'Proportion of Individuals with Diabetes by {feature}',\n",
    "                   toolbar_location=None, tools=\"\")\n",
    "        \n",
    "        # Calculate the proportion of individuals with diabetes for each category of the feature\n",
    "        diabetes_proportion = d_df.groupby([feature, 'target']).size().unstack(fill_value=0)\n",
    "        diabetes_proportion = diabetes_proportion.div(diabetes_proportion.sum(axis=1), axis=0)\n",
    "\n",
    "        # Reset the index to convert hierarchical index to columns\n",
    "        diabetes_proportion.reset_index(inplace=True)\n",
    "\n",
    "        # Convert DataFrame to ColumnDataSource\n",
    "        data = {\n",
    "            'categories': list(map(str, diabetes_proportion[feature])),\n",
    "            'Not Have Diabetes': list(diabetes_proportion[0]),\n",
    "            'Have Diabetes': list(diabetes_proportion[1])\n",
    "        }\n",
    "        source = ColumnDataSource(data=data)\n",
    "\n",
    "        # Plot stacked bars\n",
    "        p.vbar_stack(stackers=['Not Have Diabetes', 'Have Diabetes'], x='categories', width=0.9,\n",
    "                     color=color, source=source,\n",
    "                     legend_label=['Not Have Diabetes', 'Have Diabetes'])\n",
    "\n",
    "        # Add hover tooltips\n",
    "        hover = HoverTool()\n",
    "        hover.tooltips = [('Proportion', '@$name')]\n",
    "        p.add_tools(hover)\n",
    "\n",
    "        # Set plot properties\n",
    "        p.xaxis.axis_label = feature\n",
    "        p.yaxis.axis_label = 'Proportion'\n",
    "        p.legend.title = 'Diabetes Status'\n",
    "        p.legend.location = 'bottom_right'\n",
    "        p.legend.orientation = 'horizontal'\n",
    "        p.legend.label_text_font_size = '8pt'  # Set the font size of legend labels\n",
    "        p.legend.spacing = 1\n",
    "        return p\n",
    "\n",
    "# Increase the title size for each plot\n",
    "p_importance.title.text_font_size = '18pt'  # Feature Importance plot title size\n",
    "p_score.title.text_font_size = '16pt'\n",
    "\n",
    "# Create plots for each feature and add them to the list\n",
    "plots = [p_importance, p_score]  # Start with the feature importance plot\n",
    "for feature in features:\n",
    "    p = plot_feature(feature)\n",
    "    p.title.text_font_size = '16pt'  # Increase title size for other plots\n",
    "    plots.append(p)\n",
    "\n",
    "# Create a grid layout with two plots in each row\n",
    "plots_grid = [plots[i:i+2] for i in range(0, len(plots), 2)]\n",
    "\n",
    "# Combine the feature importance plot and individual feature plots into one grid\n",
    "grid = gridplot(plots_grid, sizing_mode='scale_width')\n",
    "\n",
    "# Show the grid layout\n",
    "show(grid)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c785dd95-bea8-47e2-8046-9785c8e4d44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Replace the URL below with the Tableau Public link you want to embed\n",
    "url = \"https://public.tableau.com/views/FactorsAffectingDiabetes/FactorsAffectingDiabetes?:language=en-US&:display_count=n&:origin=viz_share_link\"\n",
    "\n",
    "print(\"\\033[1m\" + \"Click here to assess the dashboard on highlighted tables for selected features\" + \"\\033[0m\")\n",
    "# HTML code to open the link in a new tab\n",
    "\n",
    "# \"Click here to assess the dashboard on highlighted tables for selected features\"\n",
    "html_code=f'<a href=\"{url}\" target=\"_blank\">Open Tableau Visualization</a>'\n",
    "\n",
    "# Display the HTML\n",
    "HTML(html_code)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cba3e7a9-fbdb-4bb5-be4c-618601d79324",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Additional \n",
    "# Define the selected features\n",
    "# Partial Dependency plot for for the binary variable\n",
    "# selected_features = ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP']\n",
    "selected_features = ['HighBP']\n",
    "\n",
    "# Initialize an empty list to store the partial dependence plots\n",
    "partial_plots = []\n",
    "\n",
    "# Loop through each selected feature\n",
    "for feature in selected_features:\n",
    "    # Get the index of the feature\n",
    "    feature_index = X_train_rf_resampled.columns.get_loc(feature)\n",
    "    \n",
    "    # Initialize a LogisticGAM model\n",
    "    gam = LogisticGAM().fit(X_train_rf_resampled.iloc[:, feature_index], Y_train_rf_resampled)\n",
    "    \n",
    "    # Check if the terms exist\n",
    "    if len(gam.terms) <= feature_index:\n",
    "        print(f\"No term found for {feature}.\")\n",
    "        continue\n",
    "    \n",
    "    # Check if the term is an intercept\n",
    "    if gam.terms[feature_index].isintercept:\n",
    "        print(f\"Skipping intercept term for {feature}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate partial dependence values\n",
    "    XX = gam.generate_X_grid(term=feature_index)\n",
    "    partial_dependence = gam.partial_dependence(term=feature_index, X=XX)\n",
    "    \n",
    "    # Plot the partial dependence\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(XX[:, feature_index], partial_dependence, label=feature)\n",
    "    plt.title(f'Partial Dependence Plot for {feature}', fontsize=16)\n",
    "    plt.xlabel(feature, fontsize=14)\n",
    "    plt.ylabel('Partial Dependence', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Append the partial dependence plot to the list\n",
    "    partial_plots.append((feature, partial_dependence))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f66585d-4639-4cf4-ad1b-3ce06e595f18",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Why we are going with a simpler model of 8 features compared to more features\n",
    "# Firstly simpler models are easier to interpret, helps avoid overfitting of the data. Additionally, simpler the model we can answer the question \"Why the model comes up with that result\"\n",
    "# Get feature importances\n",
    "feature_importances_all_resampled = rfc_all_resampled.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importance values\n",
    "feature_importance_df_all_resampled = pd.DataFrame({'Feature': X_train_resampled.columns, 'Importance': feature_importances_all_resampled})\n",
    "\n",
    "# Sort the DataFrame by importance values in descending order\n",
    "feature_importance_df_all_resampled = feature_importance_df_all_resampled.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"Feature Importance for all features using resampled data (Random Forest):\")\n",
    "print(feature_importance_df_all_resampled)\n",
    "\n",
    "print(\"\\nIt is evident that in this data set the feature importance scores are low and there can be multiple reasons corresponding to this factor.\\nHowever, except for the first 8 features, the other features in the data set doesn't have much of an impact\\n\")\n",
    "print(\"The relatively low importance scores observed for all features in this dataset could be attributed to various factors, including feature redundancy, noise in the data, and limitations in the predictive power of the selected model. While feature engineering might potentially improve the model's performance, time constraints led us to prioritize feature selection. \\nMoreover, the complexity of the model, combined with the large sample size and heavily imbalanced data, could further diminish the importance of individual features. It is essential to recognize that these factors may collectively contribute to the observed lower importance scores across all features in the dataset.\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df_all_resampled['Feature'], feature_importance_df_all_resampled['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance for all features using resampled data (Random Forest)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show most important features at the top\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
